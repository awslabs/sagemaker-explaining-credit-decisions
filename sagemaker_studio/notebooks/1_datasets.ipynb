{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Datasets\n", "\n", "In this notebook, we'll take a look at preparing datasets for machine\n", "learning using AWS Glue and create schemas to enforce validity of the\n", "data in later stages.\n", "\n", "<p align=\"center\">\n", "  <img src=\"https://github.com/awslabs/sagemaker-explaining-credit-decisions/raw/master/docs/architecture_diagrams/stage_1.png\" width=\"1000px\">\n", "</p>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When creating the AWS CloudFormation stack, a collection of synthetic\n", "datasets were generated and stored in our solution Amazon S3 bucket with\n", "a prefix of `dataset`. Most of the features contained in these datasets\n", "are based on the [German Credit\n", "Dataset](http://archive.ics.uci.edu/ml/datasets/statlog%2B%28german%2Bcredit%2Bdata%29)\n", "(UCI Machine Learning Repository), but there are some synthetic data\n", "fields too. All personal information was generated using\n", "[`Faker`](https://faker.readthedocs.io/en/master/). We have 3 datasets in\n", "total: credits, people and contacts."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #1: Credits\n", "\n", "Our credits dataset contains features directly related to the credit\n", "application.\n", "\n", "It is a CSV file (i.e. Comma Seperated Value file) that has a header row\n", "with feature names. Of particular note is the feature called `default`.\n", "It is our target variable that we're trying to predict with our LightGBM\n", "model. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"credit_id\",\"person_id\",\"amount\",\"duration\",\"purpose\",\"installment_rate\",\"guarantor\",\"coapplicant\",\"default\"\n", "\"51829372\",\"f032303d\",1169,6,\"electronics\",4,0,0,False\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #2: People\n", "\n", "Our credits data contains features related to the people making the\n", "credit applications (i.e. the applicants).\n", "\n", "It's a [JSON Lines](http://jsonlines.org/) file, where each row is a\n", "separate JSON object. Of particular note is the feature called\n", "`person_id`. You'll notice that this feature was also included in the\n", "credits dataset. It is used to connect the credit application with the\n", "applicant. We show the first row of the dataset below:\n", "\n", "```\n", "{\n", "    \"person_id\": \"f032303d\",\n", "    \"finance\": {\n", "        \"accounts\": {\n", "            \"checking\": {\n", "                \"balance\": \"negative\"\n", "            }\n", "        },\n", "        \"repayment_history\": \"very_poor\",\n", "        \"credits\": {\n", "            \"this_bank\": 2,\n", "            \"other_banks\": 0,\n", "            \"other_stores\": 0\n", "        },\n", "        \"other_assets\": \"real_estate\"\n", "    },\n", "    \"personal\": {\n", "        \"age\": 67,\n", "        \"gender\": \"male\",\n", "        \"relationship_status\": \"single\",\n", "        \"name\": \"Peter Jones\"\n", "    },\n", "    \"dependents\": [\n", "        {\n", "            \"gender\": \"male\",\n", "            \"name\": \"Michael Morales\"\n", "        }\n", "    ],\n", "    \"employment\": {\n", "        \"type\": \"professional\",\n", "        \"title\": \"Learning disability nurse\",\n", "        \"duration\": 11,\n", "        \"permit\": \"foreign\"\n", "    },\n", "    \"residence\": {\n", "        \"type\": \"own\",\n", "        \"duration\": 4\n", "    }\n", "}\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #3: Contacts\n", "\n", "Our contacts dataset contains contact information for the applicants.\n", "\n", "It is a CSV file that has a header row with feature names. Once again we\n", "have `person_id`. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"contact_id\",\"person_id\",\"type\",\"value\"\n", "\"5996e20a\",\"f032303d\",\"telephone\",\"(716)406-9514x345\"\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AWS Glue\n", "\n", "One of the most time consuming tasks in developing a machine learning\n", "workflow is data preperation. AWS Glue can be used to simplify this\n", "process. As a demonstration of how it can be used to infer data schemas\n", "and perform extract, transform and load (ETL) jobs in Spark, we'll\n", "prepare a dataset using AWS Glue. Although our sample datasets are small,\n", "there are many real world scenarios that will benefit from the\n", "scalability of AWS Glue.\n", "\n", "When creating the AWS CloudFormation stack, a number of AWS Glue resources\n", "were created:\n", "\n", "* A\n", "  [Database](https://docs.aws.amazon.com/glue/latest/dg/define-database.html)\n", "  is used to organize solution's tables.\n", "* A [Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) is\n", "  used infer formats and schemas of the datasets above.\n", "* A [Custom\n", "  Classifier](https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html)\n", "  is used to help the classifier infer the schema of the contacts datasets.\n", "    * All fields are of type 'string', so we need to indicate that the first\n", "      row is a header row rather than data.\n", "* A [Job](https://docs.aws.amazon.com/glue/latest/dg/author-job.html) is used\n", "  to join the datasets together, drop certain feature, create other features,\n", "  and split train and test sets.\n", "* A\n", "  [Workflow](https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html)\n", "  (and associated\n", "  [triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html)) to\n", "  orchestrate the above crawler and job.\n", "\n", "You can explore the service console for AWS Glue for more details."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We start by setting up the environment (e.g. install packages, etc) if this has\n", "not been done already."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "sys.path.append('../package')\n", "from package import config\n", "%cd {config.NOTEBOOKS_PATH}\n", "!python ../env_setup.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We then import a variety of packages that will be used throughout\n", "the notebook. One of the most important packages used throughout this\n", "solution is the Amazon SageMaker Python SDK (i.e. `import sagemaker`). We\n", "also import modules from our own custom package that can be found at\n", "`./package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import boto3\n", "from pathlib import Path\n", "import sagemaker\n", "import sys\n", "\n", "from package import utils\n", "from package.data import glue"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll now start the AWS Glue workflow which will first crawl the datasets\n", "stored in Amazon S3 and then execute a job for our data ELT (i.e.\n", "Extract, Transform & Load)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our workflow takes around 10 minutes to complete. Most of this time is spend\n", "on resource provisioning, but there is a [preview\n", "feature](https://pages.awscloud.com/glue-reduced-spark-times-preview-2020.html)\n", "for reduced start times. We'll wait until the AWS Glue workflow has completed\n", "before continuing. We need the dataset before training our model in Amazon\n", "SageMaker."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our AWS Glue workflow complete, we should now have 4 additional\n", "datasets in our solution's Amazon S3 bucket: `data_train`, `label_train`,\n", "`data_test` and `label_test`. We show an example first record of\n", "`data_train` below:\n", "\n", "```\n", "{\n", "\t\"credit__coapplicant\": 0,\n", "\t\"credit__guarantor\": 0,\n", "\t\"finance__credits__other_stores\": 0,\n", "\t\"employment__permit\": \"foreign\",\n", "\t\"employment__type\": \"labourer\",\n", "\t\"finance__accounts__checking__balance\": \"no_account\",\n", "\t\"finance__credits__this_bank\": 2,\n", "\t\"finance__credits__other_banks\": 1,\n", "\t\"personal__num_dependents\": 1,\n", "\t\"finance__accounts__savings__balance\": \"very_high\",\n", "\t\"residence__duration\": 2,\n", "\t\"credit__amount\": 250,\n", "\t\"contact__has_telephone\": false,\n", "\t\"employment__duration\": 4,\n", "\t\"credit__duration\": 6,\n", "\t\"finance__repayment_history\": \"very_poor\",\n", "\t\"finance__other_assets\": \"real_estate\",\n", "\t\"credit__purpose\": \"new_car\",\n", "\t\"residence__type\": \"own\",\n", "\t\"credit__installment_rate\": 2\n", "}\n", "```\n", "\n", "We now have 20 features that describe a credit application and its\n", "applicant. Since we are using JSON formatted record we still have the\n", "feature names, but additional information such as the feature types can\n", "be retrieved from the schema stored in our AWS Glue catalog. Since we're\n", "interested in explaining the model predictions, and our explanations will\n", "assign contributions to features, it's useful if our feature names are\n", "understandable.\n", "\n", "**Advanced**: We can also organize features in a hierarchy (using a seperator\n", "in the feature names), which enables summarization of the explanations. As an\n", "example, `employment__type` and `employment__duration` are both `employment`\n", "related features. We use two consecutive underscores (`__`) as our level\n", "separator."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Schema\n", "Schemas can be used to keep track of feature names, descriptions and\n", "types. Our solution uses\n", "[`jsonschema`](https://python-jsonschema.readthedocs.io/en/stable/) as\n", "the primary schema format. We have the added bonus of being able to use\n", "schemas to validate input to the trained model and deployed endpoints,\n", "and consistently map unordered dictionaries to ordered lists (as required\n", "by the model). We don't need to create schemas to use Amazon SageMaker,\n", "but they help in this solution.\n", "\n", "We already have most of this schema information in our AWS Glue catalog\n", "(it was added when data was exported by the AWS Glue Job), so let's start\n", "by retrieving the table schema for `data_train`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"data_train\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now add additional information such as feature descriptions, that will\n", "be shown inside the tooltip on the visuals later on."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# flake8: noqa: E501\n", "data_schema.title = \"Credit Application\"\n", "data_schema.description = \"An array of items used to describe a credit application.\"\n", "item_descriptions_dict = {\n", "    \"contact__has_telephone\": \"Customer has a registered telephone number.\",\n", "    \"credit__amount\": \"Amount of money requested as part of credit application (in EUR).\",\n", "    \"credit__coapplicant\": \"Co-applicant on credit application.\",\n", "    \"credit__duration\": \"Amount of time the credit is requested for (in months).\",\n", "    \"credit__guarantor\": \"Guarantor on credit application.\",\n", "    \"credit__installment_rate\": \"Credit installment rate (as a percentage of the customer's disposable income).\",\n", "    \"credit__purpose\": \"Customer's reason for requiring credit.\",\n", "    \"employment__duration\": \"Amount of time the customer has been employed at their current employer (in years).\",\n", "    \"employment__permit\": \"Customer's current work permit type.\",\n", "    \"employment__type\": \"Customer's current job classification.\",\n", "    \"finance__accounts__checking__balance\": \"Customer's checking account balance.\",\n", "    \"finance__accounts__savings__balance\": \"Customer's savings account balance.\",\n", "    \"finance__credits__other_banks\": \"Count of credits the customer has at other banks.\",\n", "    \"finance__credits__other_stores\": \"Count of credits the customer has at other stores.\",\n", "    \"finance__credits__this_bank\": \"Count of credits the customer has at this bank.\",\n", "    \"finance__other_assets\": \"Customer's most significant asset.\",\n", "    \"finance__repayment_history\": \"Quality of the customer's repayment history.\",\n", "    \"personal__num_dependents\": \"Count of the customer's dependents.\",\n", "    \"residence__duration\": \"Amount of time the customer has been at their current residence (in years).\",\n", "    \"residence__type\": \"Class of the customer's residence.\"\n", "}\n", "data_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We do the same for `label_train` too."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"label_train\"\n", ")\n", "label_schema.title = \"Credit Application Outcome\"\n", "item_descriptions_dict = {\n", "    \"credit__default\": (\n", "        \"0 if the customer successfully made credit payments, \"\n", "        \"1 if the customer defaulted on credit payments.\")\n", "}\n", "label_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since the schemas for train and test datasets are the same, we can skip\n", "`data_test` and `label_test`.\n", "\n", "We can save our updated schemas to disk, in preperation for uploading to\n", "Amazon S3. You can check the `schema_folder` afterwards, and examine the\n", "`data.schema.json` and `label.schema.json` files for data and label\n", "schemas respectively."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_folder = utils.get_current_folder(globals())\n", "schema_folder = Path(current_folder, \"../schemas\").resolve()\n", "data_schema_filepath = Path(schema_folder, \"data.schema.json\")\n", "data_schema.save(data_schema_filepath)\n", "label_schema_filepath = Path(schema_folder, \"label.schema.json\")\n", "label_schema.save(label_schema_filepath)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we create a SageMaker Session. A SageMaker Session can be used to\n", "conveniently perform certain AWS actions, such as uploading and downloading\n", "files from Amazon S3. We use the SageMaker Session to upload our schemas to\n", "Amazon S3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boto_session = boto3.session.Session(region_name=config.AWS_REGION, profile_name=config.AWS_PROFILE)\n", "sagemaker_session = sagemaker.Session(boto_session)\n", "\n", "sagemaker_session.upload_data(\n", "    path=str(schema_folder),\n", "    bucket=config.S3_BUCKET,\n", "    key_prefix=config.SCHEMAS_S3_PREFIX\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We now have data and label schemas uploaded to Amazon S3, and they can\n", "now be used during model training and for model deployment."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Customization\n", "\n", "We have provided an example dataset above, but our solution is\n", "customizable if you have your own datasets. You can choose to use AWS\n", "Glue or perform the processing steps in some other way of your choosing.\n", "\n", "When your own data still needs to be prepared for machine learning (e.g.\n", "still need to be joined and flattened to create a single table), you\n", "can modify the AWS Glue Workflow that's provided to suit your own\n", "data. As an example, if your data is stored in Amazon RDS (or another\n", "JDBC data store) rather than Amazon S3, you can add an AWS Glue\n", "Connector and configure the AWS Glue Crawler and AWS Glue Job to use it.\n", "You should also modify the AWS Glue Job's script (written in PySpark) to\n", "suit your own data (e.g. change the joins and select features of\n", "interest).\n", "\n", "When your own data is already in a suitable format for machine learning\n", "(i.e. can be represented as a single table), you don't necessarily need\n", "to run the AWS Glue Workflow. Just upload the data to Amazon S3 (in the\n", "bucket created as part of the solution). You should however convert your\n", "data to the JSON Lines format that is used by the solution. And you\n", "should create JSON schemas for the data and label to take advantage of\n", "the automatic data preprocessing and data validation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Stage\n", "\n", "Up next we'll train a LightGBM model using Amazon SageMaker, so we have\n", "an example trained model to explain.\n", "\n", "[Click here to continue.](./2_training.ipynb)"]}], "metadata": {"jupytext": {"formats": "ipynb,py:percent"}, "kernelspec": {"display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0", "language": "python", "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"}}, "nbformat": 4, "nbformat_minor": 4}