{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Training\n", "\n", "In this notebook, we'll train a LightGBM model using Amazon SageMaker, so\n", "we have an example trained model to explain.\n", "\n", "You can bring also bring your own trained models to explain. See the\n", "customizing section for more details.\n", "\n", "<p align=\"center\">\n", "  <img src=\"https://github.com/awslabs/sagemaker-explaining-credit-decisions/raw/master/docs/architecture_diagrams/stage_2.png\" width=\"1000px\">\n", "</p>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We start by importing a variety of packages that will be used throughout\n", "the notebook. One of the most important packages used throughout this\n", "solution is the Amazon SageMaker Python SDK (i.e. `import sagemaker`). We\n", "also import modules from our own custom package that can be found at\n", "`./package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from sagemaker.sklearn import SKLearn\n", "from sagemaker.local import LocalSession\n", "\n", "from package import config, utils\n", "from package.sagemaker import containers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Container\n", "We now build our custom Docker image that will be used for model training\n", "and deployment. It extends the official Amazon SageMaker framework image\n", "for Scikit-learn, by adding additional packages such as\n", "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) and\n", "[SHAP](https://github.com/slundberg/shap). After building the image, we\n", "upload it to our solution's Amazon ECR repository."]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["scikit_learn_image = containers.scikit_learn_image()\n", "custom_image = containers.custom_image()\n", "\n", "current_folder = utils.get_current_folder(globals())\n", "dockerfile = Path(current_folder, '../containers/model/Dockerfile')\n", "custom_image.build(\n", "    dockerfile=dockerfile,\n", "    buildargs={'SCIKIT_LEARN_IMAGE': str(scikit_learn_image)}\n", ")\n", "custom_image.push()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Training\n", "Amazon SageMaker provides two methods to training and deploying models.\n", "You can start by quickly testing and debuging models on the Amazon\n", "SageMaker Notebook instance using local mode. After this, you can scale\n", "up training with SageMaker mode on dedicated instances and deploy the\n", "model on dedicated instance too. Since this is a pre-developed solution\n", "we'll be using SageMaker mode."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we configure our SKLearn estimator. We will use it to coordinate\n", "model training and deployment. We reference our custom container (see\n", "`image_name`) and our custom code (see `entry_point` and `dependencies`).\n", "At this stage, we also reference the instance type (and instance count)\n", "that will be used during training, and the hyperparmeters we wish to use.\n", "And lastly we set the `output_path` for trained model artifacts and\n", "`code_location` for a snapshot of the training script that was used.\n", "\n", "**Note**: when customizing the solution, you can enable enhanced logging\n", "by setting the `container_log_level=logging.DEBUG` on the `SKLearn`\n", "estimator object (after `import logging`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hyperparameters = {\n", "    \"tree-n-estimators\": 42,\n", "    \"tree-max-depth\": 2,\n", "    \"tree-min-child-samples\": 1,\n", "    \"tree-boosting-type\": \"dart\"\n", "}\n", "\n", "estimator = SKLearn(\n", "    image_name=str(custom_image),\n", "    entry_point=str(Path(current_folder, '../containers/model/entry_point.py').resolve()),\n", "    dependencies=[str(Path(current_folder, '../package').resolve())],\n", "    hyperparameters=hyperparameters,\n", "    role=config.SAGEMAKER_IAM_ROLE,\n", "    train_instance_count=1,\n", "    train_instance_type='ml.c5.xlarge',\n", "    output_path='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX)) + '/',\n", "    code_location='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX)) + '/',\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our estimator now initialized, we can start the Amazon SageMaker\n", "training job. Since our entry point script expects a number of data\n", "channels to be defined, we can provide them when calling `fit`. When\n", "referencing `s3://` folders, the contents of these folders will be\n", "automatically downloaded from Amazon S3 before the entry point script is\n", "run. When using local mode, it's possible to avoid this data transfer and\n", "reference local folder using the `file://` prefix instead: e.g.\n", "`{'schemas': 'file://' + str(schema_folder)}`\n", "\n", "You can expect this step to take approximately 5 minutes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["estimator.fit({\n", "    'schemas': 's3://' + str(Path(config.S3_BUCKET, config.SCHEMAS_S3_PREFIX)),\n", "    'data_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_train')),\n", "    'label_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_train')),\n", "    'data_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_test')),\n", "    'label_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_test'))\n", "})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our Amazon SageMaker training job has now completed, and we should have a\n", "number of trained model artifacts that can be deployed and used for\n", "explanations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Customization\n", "\n", "We have provided an example of model training above, so that we have a\n", "trained model to explain, but our solution is customizable if you have\n", "your own models. You can choose to retrain your models on Amazon\n", "SageMaker or train your models is some other way of your choosing.\n", "\n", "When re-training models on Amazon SageMaker you should modify the\n", "training script found at `./package/sagemaker/estimator_fns.py`. You\n", "should modify the `train_fn` function as required and change any of the\n", "other training functions (found in `./package/machine_learning` for\n", "example). You may need to modify the dependencies too depending on you\n", "model and these can be adjusted in `./containers/model/requirements.txt`.\n", "\n", "When bringing your own trained model, you will need to upload all of the\n", "model assets to Amazon S3 (in the solution bucket): e.g. trained\n", "preprocessors, model weights and feature schemas (i.e. data schema after\n", "feature engineering). Amazon SageMaker expects all of these model assets\n", "to be packages up as a `model.tar.gz`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Stage\n", "\n", "Up next we'll deploy the model explainer to a HTTP endpoint using Amazon\n", "SageMaker and visualize the explanations.\n", "\n", "[Click here to continue.](./3_endpoint.ipynb)"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}}, "nbformat": 4, "nbformat_minor": 4}